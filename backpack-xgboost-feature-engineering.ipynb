{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90274,"databundleVersionId":10995111,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aaronisomaisom3/backpack-xgboost-feature-engineering?scriptVersionId=254610601\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"#!pip install xgbtune","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:01:51.091235Z","iopub.execute_input":"2025-02-28T20:01:51.091499Z","iopub.status.idle":"2025-02-28T20:01:51.094974Z","shell.execute_reply.started":"2025-02-28T20:01:51.091476Z","shell.execute_reply":"2025-02-28T20:01:51.094272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary modules\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve, validation_curve, cross_val_score\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom category_encoders import TargetEncoder, CatBoostEncoder\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport itertools\n\n# Uncomment for hyperparameter tuning\n# from xgbtune import tune_xgb_model \n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:01:51.099408Z","iopub.execute_input":"2025-02-28T20:01:51.099703Z","iopub.status.idle":"2025-02-28T20:01:52.486708Z","shell.execute_reply.started":"2025-02-28T20:01:51.099669Z","shell.execute_reply":"2025-02-28T20:01:52.485615Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This notebook attempts to explore using XGBoost Regression for the Backpack Prediction Challenge. It is my first attempt at a public Kaggle competition, so I welcome any feedback as I have \"learned\" so much during this exercise and the awesome Kaggle community. Thanks for the help!\n\nMy analysis focuses on the following areas:\n* Basic EDA to understand the data;\n* Imputation of missing values using \"Unknown\";\n* Feature engineering using log transformation and Mean() to avoid data leakage issues;\n* Uses CatBoostEncoder for categorical feature encoding (better for tree models).\n* Limit new interaction features to a max of 10 (prevents feature explosion).\n* Use SelectKBest to choose only the X best features (faster than SHAP or KFold).\n* Encoding categorical features using CatBoost_Encoder();\n* XGBTune and Randomized Grid Search for tuning best hyperparameters;\n* Comparing XGBoost vs Generic CatBoost using Root Mean Squared Error (RMSE);\n* Exploring XGBoost feature importances.\n\nAlso, I attempted encoding using both CatBoost_Encoder and Target_Encoder using category_encoders. Target encoding provided okay results with XGBoost. Meanwhile, a generic implementation of CatBoost regression returned a low RMSE in only 100 iterations during baseline testing. Therefore, CatBoost likely has room for improvement with proper hyperparameter tuning since it natively handles categorical features.\n\nFuture areas to explore further include:\n* Explore better pruning strategy using min_split_loss (gamma) and max_depth;\n* Localized RMSE scores are around 38.4, which indicates overfitting since the submitted score is around 39.095;\n* Investigate more ensembles using CatBoost with XGBoost and LightGBM, which might capture patterns more efficiently;\n* Explore CatBoost further since it natively handles categorical features better than the TargetEncoder() + XGBoost combination;\n* The Backpack data set is noisy, so more time should be spent on a strategy around feature engineering, missing values, and imputation.\n\nThanks to Chris Deotte's detailed explanation of the weight capacity numerical feature and discussion of synthetic data. https://www.kaggle.com/code/cdeotte/two-baseline-models-lb-38-91\n","metadata":{}},{"cell_type":"markdown","source":"Load the Train, Train Extra, Test, and Submission data sets. Then, concatenate the Train + Train Extra.","metadata":{}},{"cell_type":"code","source":"# Load the data\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s5e2/train.csv\", index_col='id')\ntrain_extra = pd.read_csv(\"/kaggle/input/playground-series-s5e2/training_extra.csv\", index_col='id')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e2/test.csv', index_col='id')\n\n# Combine train and train extra data sets into one\ndf = pd.concat([train, train_extra], axis=0, ignore_index=True)\n\n# Rows and Cols\ndisplay(df.shape)\ndisplay(test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:01:52.48778Z","iopub.execute_input":"2025-02-28T20:01:52.488316Z","iopub.status.idle":"2025-02-28T20:01:59.440622Z","shell.execute_reply.started":"2025-02-28T20:01:52.488278Z","shell.execute_reply":"2025-02-28T20:01:59.439832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A quick analysis of the data indicates lots of noise with missing values in categorical features.","metadata":{}},{"cell_type":"code","source":"# Descriptive stats\ndf.describe()\n\n# Columns and \ndf.info()\ntest.info()\n\n# Data types\ndisplay(df.dtypes)\n\n#Summarize\ndf.head()\n\n# Check for null/missing values\ndisplay(df.dtypes, df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:01:59.441522Z","iopub.execute_input":"2025-02-28T20:01:59.441816Z","iopub.status.idle":"2025-02-28T20:02:01.464448Z","shell.execute_reply.started":"2025-02-28T20:01:59.441791Z","shell.execute_reply":"2025-02-28T20:02:01.463654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot price distributions\nplt.figure(figsize=(10, 5))\nsns.histplot(df[\"Price\"], bins=50, kde=True, color=\"green\")\nplt.title(\"Price Distribution\")\nplt.show()\n\n# Compare distributions between train and test\nsns.kdeplot(df['Weight Capacity (kg)'], label=\"Train\", shade=True)\nsns.kdeplot(test['Weight Capacity (kg)'], label=\"Test\", shade=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:02:01.465241Z","iopub.execute_input":"2025-02-28T20:02:01.465513Z","iopub.status.idle":"2025-02-28T20:02:36.59983Z","shell.execute_reply.started":"2025-02-28T20:02:01.465488Z","shell.execute_reply":"2025-02-28T20:02:36.598818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, create X and y by dropping Price from the training dataset.","metadata":{}},{"cell_type":"code","source":"# Drop the target Price column\nX = df.drop(columns=['Price'])\ny = df['Price']\ndisplay(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:02:36.600969Z","iopub.execute_input":"2025-02-28T20:02:36.601307Z","iopub.status.idle":"2025-02-28T20:02:36.782081Z","shell.execute_reply.started":"2025-02-28T20:02:36.60128Z","shell.execute_reply":"2025-02-28T20:02:36.781269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature engineering function to determine better feature relationships/correlations.\ndef feature_engineering_xgb(X_train, y_train, test, cat_features, top_corr_features=5, max_new_features=10, k_best=10):\n    \"\"\"\n    Enhances dataset by:\n    1. Handling missing values.\n    2. Encoding categorical features using CatBoostEncoder.\n    3. Identifying top correlated numerical features with Price (train only).\n    4. Generating limited interaction features (train & test).\n    5. Removing duplicate features.\n    6. Selecting the best features using SelectKBest.\n\n    Parameters:\n    - X_train: Training Feature DataFrame\n    - y_train: Training target variable (Price)\n    - test: Test Feature DataFrame (no y (Price) available)\n    - cat_features: List of categorical feature names\n    - top_corr_features: Number of top correlated numerical features to use in interactions\n    - max_new_features: Maximum number of new features to generate\n    - k_best: Number of top features to select with SelectKBest\n\n    Returns:\n    - Transformed `X_train` and `test` DataFrames with engineered features.\n    - List of selected features from training data.\n    \"\"\"\n   \n    # Copy data to prevent modification\n    X_train = X_train.copy()\n    test = test.copy()\n\n    print(\"\\nüõ†Ô∏è Handling Missing Values...\")\n\n    # Show NaN counts before handling\n    print(\"\\nüîç NaN Counts Before Handling:\")\n    print(\"Training Data:\\n\", X_train.isna().sum().sort_values(ascending=False).head(10))\n    print(\"Test Data:\\n\", test.isna().sum().sort_values(ascending=False).head(10))\n\n    # Fill categorical NaN with \"Unknown\" (Train & Test)\n    X_train[cat_features] = X_train[cat_features].fillna(\"Unknown\")\n    test[cat_features] = test[cat_features].fillna(\"Unknown\")\n    \n    # Log Transformation (Train & Test)\n    X_train['Weight Capacity (kg)'] = X_train['Weight Capacity (kg)'].fillna(X_train['Weight Capacity (kg)'].mean())\n    X_train['Compartments'] = X_train['Compartments'].fillna(X_train['Compartments'].mean())\n    \n    test['Weight Capacity (kg)'] = test['Weight Capacity (kg)'].fillna(test['Weight Capacity (kg)'].mean())\n    test['Compartments'] = test['Compartments'].fillna(test['Compartments'].mean())\n\n    # Show NaN counts after handling\n    print(\"\\n‚úÖ NaN Counts After Handling:\")\n    print(\"Training Data:\\n\", X_train.isna().sum().sort_values(ascending=False).head(10))\n    print(\"Test Data:\\n\", test.isna().sum().sort_values(ascending=False).head(10))\n    print(\"‚úÖ Filled missing values in categorical and numerical features.\")\n\n    # Step 2: Encode Categorical Features Using CatBoostEncoder\n    print(\"\\nüî¢ Applying CatBoost Encoding to Categorical Features...\")\n    encoder = CatBoostEncoder(cols=cat_features, random_state=42)\n    X_train_encoded = encoder.fit_transform(X_train, y_train)\n    test_encoded = encoder.transform(test)\n\n    print(f\"‚úÖ CatBoost Encoded {len(cat_features)} categorical features.\")\n\n    # Step 3: Compute Correlations (Train Only)\n    corr_matrix = X_train_encoded.corrwith(y_train).abs().sort_values(ascending=False)\n    print(\"\\nüîç Top 10 Features Most Correlated with Price:\\n\", corr_matrix.head(10))\n\n    # Select top correlated numerical features for interactions (Train Only)\n    top_features = corr_matrix.head(top_corr_features).index.tolist()\n    numeric_cols = X_train_encoded[top_features].select_dtypes(include=[np.number]).columns.tolist()\n\n   # Step 4: Generate LIMITED Interaction Features (Train & Test)\n    print(\"\\n‚öôÔ∏è Generating Limited Interaction Features...\")\n    new_features = []\n    interaction_count = 0  # Track number of generated features\n\n    for col1, col2 in itertools.combinations(numeric_cols, 2):\n        if interaction_count >= max_new_features:\n            break  # Stop generating new features if max_new_features is reached\n\n        X_train_encoded[f\"{col1}_x_{col2}\"] = X_train_encoded[col1] * X_train_encoded[col2]\n        X_train_encoded[f\"{col1}_div_{col2}\"] = X_train_encoded[col1] / (X_train_encoded[col2] + 1e-6)  # Avoid division by zero\n        test_encoded[f\"{col1}_x_{col2}\"] = test_encoded[col1] * test_encoded[col2]\n        test_encoded[f\"{col1}_div_{col2}\"] = test_encoded[col1] / (test_encoded[col2] + 1e-6)\n\n        new_features.extend([f\"{col1}_x_{col2}\", f\"{col1}_div_{col2}\"])\n        interaction_count += 2\n\n    print(f\"‚úÖ Created {interaction_count} new interaction features.\")\n\n    # Step 5: Remove Duplicate Features (Train & Test)\n    print(\"\\nüóëÔ∏è Removing Duplicate Features...\")\n    X_train_encoded = X_train_encoded.loc[:, ~X_train_encoded.columns.duplicated()]\n    test_encoded = test_encoded.loc[:, ~test_encoded.columns.duplicated()]\n    print(f\"‚úÖ Removed duplicate features. Remaining Train Features: {X_train_encoded.shape[1]}, Test Features: {test_encoded.shape[1]}\")\n\n    # Step 6: Feature Selection Using SelectKBest\n    print(\"\\nüîç Selecting Best Features Using SelectKBest...\")\n    selector = SelectKBest(score_func=f_regression, k=k_best)\n    X_train_selected = selector.fit_transform(X_train_encoded, y_train)\n    test_selected = selector.transform(test_encoded)\n    selected_features = X_train_encoded.columns[selector.get_support()].tolist()\n    print(f\"\\nüéØ Selected Top {len(selected_features)} Features Based on SelectKBest:\")\n    print(selected_features, \"\\n\\n\")\n\n    # Convert to DataFrame\n    X_train_selected = pd.DataFrame(X_train_selected, columns=selected_features, index=X_train_encoded.index)\n    test_selected = pd.DataFrame(test_selected, columns=selected_features, index=test_encoded.index)\n\n    return X_train_selected, test_selected, selected_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:02:36.784403Z","iopub.execute_input":"2025-02-28T20:02:36.784661Z","iopub.status.idle":"2025-02-28T20:02:36.797671Z","shell.execute_reply.started":"2025-02-28T20:02:36.78464Z","shell.execute_reply":"2025-02-28T20:02:36.796634Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Identify the \"best\" hyperparameters for the XGBRegressor model. RandomizedSearchCV is used instead of typical GridSearchCV to reduce computational complexity and run time. Optuna is also added but not fully implemented yet.","metadata":{}},{"cell_type":"code","source":"# Function to find best hyperparameters to tune the XGBRegressor.\ndef randomized_grid_search(X_train, y_train):\n    xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n    \n    params = {\n        'n_estimators': [100, 500, 1000],\n        'learning_rate': [0.01, 0.05, 0.25, 0.1],\n        'max_depth': [1, 3, 5, 7, 9],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.4, 0.6, 0.8],\n        'colsample_bytree': [0.5, 0.8, 0.9],\n        'reg_lambda': [0, 1, 5, 10],\n        'min_split_loss': [0, 0.1, 0.5, 1, 3, 5, 10],\n        'reg_alpha': [0, 0.1, 1]\n    }\n    \n    grid_search = RandomizedSearchCV(xgb, params, cv=5, random_state=42,\n                                     scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n    \n    grid_search.fit(X_train, y_train)\n    print(\"Best parameters:\", grid_search.best_params_)\n    print(\"Best Score:\", grid_search.best_score_)\n\n    return grid_search.best_params_, grid_search.best_score_\n\n# Do a random grid search to understand the impact of the hyperparameters.\n# Commented out since this takes LOTS of time. \n# randomized_grid_search(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:02:36.799062Z","iopub.execute_input":"2025-02-28T20:02:36.799338Z","iopub.status.idle":"2025-02-28T20:02:36.816227Z","shell.execute_reply.started":"2025-02-28T20:02:36.799314Z","shell.execute_reply":"2025-02-28T20:02:36.815366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Explore xgbtune to determine the best hyperparameters\n# import xgboost as xgb\n# best_params = {'eval_metric': 'rmse', 'tree_method': 'hist'}\n\n# params, round_count = tune_xgb_model(best_params, X_train_transformed, y)\n\n# new_Train = xgb.DMatrix(X_train_transformed, label=y)\n# xgb_model = xgb.train(params, new_Train, num_boost_round=round_count)\n\n# Final model prediction\n# final_Test = xgb.DMatrix(X_test_transformed)\n# final_test_pred = xgb_model.predict(final_Test)\n# display(pd.DataFrame({'id': test.index, 'Price': final_test_pred}))\n\n# RMSE:38.3 {'eval_metric': 'rmse', 'tree_method': 'hist', 'max_depth': 8, 'min_child_weight': 2, 'gamma': 0.0, 'subsample': 0.95, 'colsample_bytree': 1.0, 'alpha': 0.1, 'lambda': 1, 'seed': 0}\n# RMSE: 38.4 {'eval_metric': 'rmse', 'tree_method': 'hist', 'max_depth': 8, 'min_child_weight': 1, 'gamma': 0.0, 'subsample': 0.95, 'colsample_bytree': 1.0, 'alpha': 1, 'lambda': 1, 'seed': 0}\n# RMSE: 38.8 {'eval_metric': 'rmse', 'tree_method': 'hist', 'max_depth': 8, 'min_child_weight': 1, 'gamma': 0.0, 'subsample': 1.0, 'colsample_bytree': 1.0, 'alpha': 0, 'lambda': 1, 'seed': 0}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:02:36.817079Z","iopub.execute_input":"2025-02-28T20:02:36.81739Z","iopub.status.idle":"2025-02-28T20:02:36.830112Z","shell.execute_reply.started":"2025-02-28T20:02:36.817357Z","shell.execute_reply":"2025-02-28T20:02:36.829382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To check model performance, perform XGBoost regression using tuned hyperparameters with output every 100 iterations. Early stopping is added to avoid overfitting. XGBoost tuning was performed using xgbtune, which is relatively quick compared to a traditional grid search.","metadata":{}},{"cell_type":"code","source":"cat_features = ['Size', 'Brand', 'Material', 'Style', 'Color', 'Laptop Compartment', 'Waterproof']\nX_train_transformed, X_test_transformed, selected_features = feature_engineering_xgb(X, y, test, cat_features)\n\nX_train, X_test, y_train, y_test = train_test_split(X_train_transformed, y, test_size=0.2, random_state=42, shuffle=True)\n\nbest_params = {'eval_metric': 'rmsle', 'tree_method': 'hist', 'max_depth': 8, 'min_child_weight': 2,  \n               'subsample': 0.8, 'colsample_bytree': 1.0, 'alpha': 0, 'lambda': 1, 'seed': 0, 'random_state': 42}\n\n# Run XGB given the tuned parameters\nxgb_boost = XGBRegressor(early_stopping_rounds=100, **best_params)\n\nxgb_boost.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=100)\nprint(f\"Best XGBoost iteration using RMSE: {xgb_boost.best_iteration} \\n\")\n\ny_pred = xgb_boost.predict(X_test)\n\n# Compare MAE & RMSE\nmae = mean_absolute_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(f\"üìä Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"üìä Root Mean Squared Error (RMSE): {rmse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:02:36.830912Z","iopub.execute_input":"2025-02-28T20:02:36.831213Z","iopub.status.idle":"2025-02-28T20:03:29.725703Z","shell.execute_reply.started":"2025-02-28T20:02:36.831173Z","shell.execute_reply":"2025-02-28T20:03:29.724685Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CatBoost is also included for comparison with XGBoost, but is not tuned.","metadata":{}},{"cell_type":"code","source":"# Try CatBoost for comparison\n#cbr = CatBoostRegressor(iterations=500,\n#                        depth=5,\n#                        learning_rate=0.3,\n#                        loss_function='RMSE',\n#                        random_state=42)\n\n#cbr.fit(X_train_encoded, y_train, eval_set=[(X_test_encoded, y_test)], verbose=200)\n#y_pred = cbr.predict(X_test_encoded)\n#rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n#print(f\"CatBoost Root Mean Squared Error (RMSE): {rmse}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:03:29.726793Z","iopub.execute_input":"2025-02-28T20:03:29.727125Z","iopub.status.idle":"2025-02-28T20:03:29.730571Z","shell.execute_reply.started":"2025-02-28T20:03:29.727092Z","shell.execute_reply":"2025-02-28T20:03:29.72966Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Prepare the final submission file with ID and predicted Price values.","metadata":{}},{"cell_type":"code","source":"# Handle the prediction and submission file generation\ntest_pred = xgb_boost.predict(X_test_transformed)\n\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e2/sample_submission.csv')\nsubmission = pd.DataFrame({'id': submission.id, 'Price': test_pred})\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\ndisplay(submission)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:03:29.731585Z","iopub.execute_input":"2025-02-28T20:03:29.731833Z","iopub.status.idle":"2025-02-28T20:03:30.126713Z","shell.execute_reply.started":"2025-02-28T20:03:29.731811Z","shell.execute_reply":"2025-02-28T20:03:30.125592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove old file(s)\n# import os\n# os.remove('/kaggle/working/submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:03:30.127904Z","iopub.execute_input":"2025-02-28T20:03:30.128388Z","iopub.status.idle":"2025-02-28T20:03:30.132469Z","shell.execute_reply.started":"2025-02-28T20:03:30.128346Z","shell.execute_reply":"2025-02-28T20:03:30.131396Z"}},"outputs":[],"execution_count":null}]}